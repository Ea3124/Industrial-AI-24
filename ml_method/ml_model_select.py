# -*- coding: utf-8 -*-
"""
ë°ì´í„° ì‚­ì œí•˜ëŠ” ë°©ì‹ì˜ ì „ì²˜ë¦¬ ì§„í–‰
"""

import joblib
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import optuna
import numpy as np
from datetime import datetime

def prepare_merged_data(df_hourly, climate_processed, use_predicted_sensor=False):
    """
    ë°œì „ê¸° ë°ì´í„°ì™€ ê¸°í›„ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    """
    df = df_hourly.reset_index().merge(
        climate_processed,
        how='inner',
        left_on='Date',
        right_on='time'
    )

    # ì„¼ì„œ ê´€ë ¨ ì—´ ì œê±°
    eng_col_remove = [
        'Date', 
        'time', 
        'lon', 
        'lat', 
        'Generator_Running_State',
        'Generation_Amount',
        'Sensor_Wind_Speed',
        'Generator_Current_A', 
        'Generator_Current_B', 
        'Generator_Current_C',
        'Gear_Oil_Temperature', 
        'Generator_Output', 
        'Generator_Speed',
        'Internal_Temperature', 
        'Coolant_Temperature', 
        'Hydraulic_Oil_Temperature',
        'Pitch_Pressure', 
        'Rotor_Speed',
        'Winding_Temperature_A', 
        'Winding_Temperature_B', 
        'Winding_Temperature_C',
        'Transformer_Temperature_BUS', 
        'Transformer_Temperature_A', 
        'Transformer_Temperature_B', 
        'Transformer_Temperature_C',
        'Generator_Voltage_A', 
        'Generator_Voltage_B', 
        'Generator_Voltage_C'
    ]

    y = df['Generation_Amount']

    if use_predicted_sensor:
        cols_to_drop = eng_col_remove
    else:
        cols_to_drop = eng_col_remove + ['Predicted_Sensor_Wind_Speed']

    X = df.drop(columns=cols_to_drop, errors='ignore')  # errors='ignore'ë¡œ ì•ˆì „í•˜ê²Œ
    return X, y

def train_and_evaluate_model(model, X, y, name='Model'):
    """
    ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜
    """
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, shuffle=False  # ì‹œê³„ì—´ ìœ ì§€
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    mae = mean_absolute_error(y_val, y_pred)
    print(f"{name} MAE: {mae:.4f}")
    return model, mae

def train_and_evaluate(X, y, name='Model'):
    """
    ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜ (ëª¨ë¸ ê³ ì •)
    """
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, shuffle=False  # ì‹œê³„ì—´ì€ ì‹œê°„ ìˆœì„œ ìœ ì§€
    )

    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)

    mae = mean_absolute_error(y_val, y_pred)
    print(f"{name} MAE: {mae:.4f}")
    return model, mae

def plot_feature_importance(model, feature_names, top_n=19):
    """
    íŠ¹ì„± ì¤‘ìš”ë„ ê·¸ë˜í”„ í•¨ìˆ˜
    """
    importances = model.feature_importances_
    sorted_idx = importances.argsort()[::-1]
    top_features = sorted_idx[:top_n]

    plt.figure(figsize=(10, 6))
    plt.barh(range(top_n), importances[top_features][::-1])
    plt.yticks(range(top_n), [feature_names[i] for i in top_features][::-1])
    plt.xlabel('Importance')
    plt.title('Top Feature Importances')
    plt.tight_layout()
    plt.show()

def get_top_features(model, feature_names, top_n=10):
    """
    íŠ¹ì„± ì¤‘ìš”ë„ ìƒìœ„ top_nê°œ ì»¬ëŸ¼ ë°˜í™˜ í•¨ìˆ˜
    """
    importances = model.feature_importances_
    sorted_idx = importances.argsort()[::-1]
    return [feature_names[i] for i in sorted_idx[:top_n]]


# # ---------- â‘  Model A: RandomForest ----------
# X_a, y_a = prepare_merged_data(df_hourly, climate_processed, use_predicted_sensor=True)
# model_rt, mae_rt = train_and_evaluate(X_a, y_a, name='Model A (No Sensor_Wind_Speed)')

# # ---------- â‘¡ Model B: XGBoost ----------
# xgb_model = XGBRegressor(n_estimators=100, random_state=42)
# model_xgb, mae_xgb = train_and_evaluate_model(xgb_model, X_a, y_a, name='Model B (XGBoost)')

# # ---------- â‘¢ Model C: LightGBM ----------
# lgbm_model = LGBMRegressor(n_estimators=100, random_state=42)
# model_lgbm, mae_lgbm = train_and_evaluate_model(lgbm_model, X_a, y_a, name='Model C (LightGBM)')

# # ---------- â‘¢ Model D: ExtraTree ----------
# et_model = ExtraTreesRegressor(n_estimators=100, random_state=42)
# model_et, mae_et = train_and_evaluate_model(et_model, X_a, y_a, name='Model D (ExtraTrees)')

# # plot_feature_importance(model_a, X_a.columns)
# print("\nğŸ“Š MAE ë¹„êµ")
# print(f"RandomForest : {mae_rt:.4f}")
# print(f"XGBoost      : {mae_xgb:.4f}")
# print(f"LightGBM     : {mae_lgbm:.4f}")
# print(f"ExtraTree    : {mae_et:.4f}")


# Target log ë³€í™˜
# y_log = np.log1p(y_a)

# # í›ˆë ¨/ê²€ì¦ ë‚˜ëˆ„ê¸° (ì‹œê³„ì—´ ìˆœì„œ ìœ ì§€)
# X_train, X_val, y_train_log, y_val_log = train_test_split(X_a, y_log, test_size=0.2, shuffle=False)

def objective_rf(trial):
    """
    Random Forest ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
    """
    model = RandomForestRegressor(
        n_estimators=trial.suggest_int('n_estimators', 50, 300),
        max_depth=trial.suggest_int('max_depth', 3, 20),
        min_samples_split=trial.suggest_int('min_samples_split', 2, 10),
        min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 10),
        max_features=trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
        random_state=42
    )
    model.fit(X_train, y_train_log)
    y_pred = np.expm1(model.predict(X_val))
    return mean_absolute_error(np.expm1(y_val_log), y_pred)

def objective_xgb(trial):
    """
    XGBoost ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
    """
    model = XGBRegressor(
        n_estimators=trial.suggest_int('n_estimators', 50, 300),
        max_depth=trial.suggest_int('max_depth', 3, 15),
        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),
        subsample=trial.suggest_float('subsample', 0.5, 1.0),
        colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1.0),
        random_state=42,
        verbosity=0
    )
    model.fit(X_train, y_train_log)
    y_pred = np.expm1(model.predict(X_val))
    return mean_absolute_error(np.expm1(y_val_log), y_pred)

def objective_lgbm(trial):
    """
    LightGBM ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
    """
    model = LGBMRegressor(
        n_estimators=trial.suggest_int('n_estimators', 50, 300),
        max_depth=trial.suggest_int('max_depth', 3, 15),
        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),
        num_leaves=trial.suggest_int('num_leaves', 20, 150),
        subsample=trial.suggest_float('subsample', 0.5, 1.0),
        colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1.0),
        random_state=42
    )
    model.fit(X_train, y_train_log)
    y_pred = np.expm1(model.predict(X_val))
    return mean_absolute_error(np.expm1(y_val_log), y_pred)

def objective_et(trial):
    """
    ExtraTrees ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
    """
    model = ExtraTreesRegressor(
        n_estimators=trial.suggest_int('n_estimators', 50, 300),
        max_depth=trial.suggest_int('max_depth', 3, 20),
        min_samples_split=trial.suggest_int('min_samples_split', 2, 10),
        min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 10),
        max_features=trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
        random_state=42
    )
    model.fit(X_train, y_train_log)
    y_pred = np.expm1(model.predict(X_val))
    return mean_absolute_error(np.expm1(y_val_log), y_pred)


# Random Forest
# study_rf = optuna.create_study(direction='minimize')
# study_rf.optimize(objective_rf, n_trials=300)
# print("âœ… RF Best MAE:", study_rf.best_value)

# # XGBoost
# study_xgb = optuna.create_study(direction='minimize')
# study_xgb.optimize(objective_xgb, n_trials=300)
# print("âœ… XGB Best MAE:", study_xgb.best_value)

# # LightGBM
# study_lgbm = optuna.create_study(direction='minimize')
# study_lgbm.optimize(objective_lgbm, n_trials=300)
# print("âœ… LGBM Best MAE:", study_lgbm.best_value)

# # ExtraTrees
# study_et = optuna.create_study(direction='minimize')
# study_et.optimize(objective_et, n_trials=300)
# print("âœ… ET Best MAE:", study_et.best_value)


# results = {
#     "RandomForest": {
#         "mae": study_rf.best_value,
#         "params": study_rf.best_params
#     },
#     "XGBoost": {
#         "mae": study_xgb.best_value,
#         "params": study_xgb.best_params
#     },
#     "LightGBM": {
#         "mae": study_lgbm.best_value,
#         "params": study_lgbm.best_params
#     },
#     "ExtraTrees": {
#         "mae": study_et.best_value,
#         "params": study_et.best_params
#     }
# }

# print("ğŸ“Š Best MAEs:", results)

# Best MAEs: {
# 'RandomForest': {'mae': 299.6347434097198, 'params': {'n_estimators': 287, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': None}}, 
# 'XGBoost': {'mae': 295.4434350211665, 'params': {'n_estimators': 152, 'max_depth': 11, 'learning_rate': 0.13200740123317442, 'subsample': 0.9990413522421722, 'colsample_bytree': 0.7618993936494004}}, 
# 'LightGBM': {'mae': 300.0865091744985, 'params': {'n_estimators': 72, 'max_depth': 13, 'learning_rate': 0.014357764389799507, 'num_leaves': 26, 'subsample': 0.9722508599365528, 'colsample_bytree': 0.871686254214325}}, 
# 'ExtraTrees': {'mae': 300.94075733053677, 'params': {'n_estimators': 244, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}}}

# import joblib
# from xgboost import XGBRegressor

# # â‘  ìµœì  params
# best_params_xgb = {
#     'colsample_bytree': 0.7618993936494004,
#     'learning_rate': 0.13200740123317442,
#     'max_depth': 11,
#     'n_estimators': 152,
#     'subsample': 0.9990413522421722
# }

# # â‘¡ ì „ì²´ í•™ìŠµìš© ë°ì´í„°
# X_all, y_all = prepare_merged_data(df_hourly, climate_processed, use_predicted_sensor=True)

# # â‘¢ ì¬í•™ìŠµ
# final_model_xgb = XGBRegressor(**best_params_xgb, random_state=42)
# final_model_xgb.fit(X_all, y_all)

# # â‘£ ëª¨ë¸ ì €ì¥
# joblib.dump(final_model_xgb, 'xgb_final_model.pkl')
# print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: xgb_final_model.pkl")


def predict_generation_from_test(climate_test_df, model_path='xgb_final_model.pkl'):
    """
    ì €ì¥ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€, test ê¸°í›„ ë°ì´í„°ì— ëŒ€í•´ ë°œì „ëŸ‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    
    Parameters
    ----------
    climate_test_df : pd.DataFrame
        23~25ì¼ ê¸°í›„ ì „ì²˜ë¦¬ ë°ì´í„°
    model_path : str
        ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ
    
    Returns
    -------
    pd.DataFrame
        ì‹œê°„ & ì˜ˆì¸¡ ë°œì „ëŸ‰ í¬í•¨ëœ DataFrame
    """
    # 1. ëª¨ë¸ ë¡œë“œ
    model = joblib.load(model_path)

    # 2. ë°œì „ëŸ‰ í•™ìŠµìš© ì»¬ëŸ¼ ì œì™¸
    df_input = climate_test_df.drop(columns=['lon', 'lat', 'time'], errors='ignore')

    # 3. ì˜ˆì¸¡
    df_input = df_input.drop(columns=['surface zodo len'], errors='ignore')
    y_pred = model.predict(df_input)

    # 4. ê²°ê³¼ df
    result = climate_test_df[['time']].copy()
    result.rename(columns={'time': 'ì‹œê°„'}, inplace=True)
    result['ë°œì „ëŸ‰'] = y_pred

    return result



